# breast_cancer_streamlit_app.py
import streamlit as st
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
import hashlib
from torchvision import models
from PIL import Image
from lime import lime_image
from skimage.segmentation import mark_boundaries
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

# Advanced Explainability imports
from captum.attr import (
    GradientShap, 
    Saliency, 
    LayerGradCam,
    LayerLRP,
    IntegratedGradients,
    NoiseTunnel
)
from captum.attr import visualization as viz
import matplotlib.cm as cm
import base64
import io

# Fairness and Bias Detection imports
from fairlearn.postprocessing import ThresholdOptimizer
from fairlearn.metrics import MetricFrame, equalized_odds_difference, demographic_parity_difference
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

# Paths
TRAIN_DIR = "train"
VAL_DIR = "val"
TEST_DIR = "test"

# Blockchain Logger
class BlockchainLogger:
    def __init__(self):
        self.chain = []
        self.create_block(proof=1, previous_hash='0')

    def create_block(self, proof, previous_hash, data=None):
        block = {
            'index': len(self.chain) + 1,
            'timestamp': datetime.now().isoformat(),
            'proof': proof,
            'previous_hash': previous_hash,
            'data': data or {}
        }
        self.chain.append(block)
        return block

    def log_ai_metrics(self, model_name, metrics):
        last_block = self.chain[-1]
        new_block = self.create_block(
            proof=hashlib.sha256(json.dumps(metrics).encode()).hexdigest(),
            previous_hash=self.hash_block(last_block),
            data={
                'model': model_name,
                'metrics': metrics
            }
        )
        return new_block

    @staticmethod
    def hash_block(block):
        return hashlib.sha256(json.dumps(block).encode()).hexdigest()
    
    def get_prediction_stats(self):
        """Get statistics for bias detection"""
        predictions = []
        for block in self.chain[1:]:  # Skip genesis block
            if 'data' in block and 'metrics' in block.get('data', {}):
                metrics = block['data']['metrics']
                prediction_data = {
                    'timestamp': block.get('timestamp'),
                    'prediction': metrics.get('prediction'),
                    'confidence': metrics.get('confidence'),
                    'filename': metrics.get('filename'),
                    'block_index': block.get('index')
                }
                # Include image data if available
                if 'image_data' in metrics:
                    prediction_data['image_data'] = metrics['image_data']
                predictions.append(prediction_data)
        return predictions

# Load model (pretrained ResNet18 with fine-tuning)
@st.cache_resource
def load_model():
    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, 2)
    model.load_state_dict(torch.load("best_model.pt", map_location=torch.device('cpu')))
    model.eval()
    return model

# Transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# Class names
class_names = ['benign', 'malignant']

# LIME Explainer
@st.cache_resource
def load_lime_explainer():
    explainer = lime_image.LimeImageExplainer()
    return explainer

# Advanced Explainability Methods
@st.cache_resource
def load_explainability_methods():
    """Initialize various explainability methods"""
    methods = {}
    return methods

def generate_saliency_map(model, image_tensor, target_class):
    """Generate saliency map using Captum"""
    saliency = Saliency(model)
    attribution = saliency.attribute(image_tensor, target=target_class)
    return attribution

def generate_gradcam(model, image_tensor, target_class, target_layer='layer4'):
    """Generate GradCAM using Captum"""
    # Get the target layer
    if target_layer == 'layer4':
        layer = model.layer4[1].conv2
    else:
        layer = model.layer4[0].conv1
    
    layer_gc = LayerGradCam(model, layer)
    attribution = layer_gc.attribute(image_tensor, target=target_class)
    return attribution

def generate_lrp(model, image_tensor, target_class):
    """Generate Layer-wise Relevance Propagation"""
    try:
        # Use the last convolutional layer for LRP
        layer = model.layer4[1].conv2
        lrp = LayerLRP(model, layer)
        attribution = lrp.attribute(image_tensor, target=target_class)
        return attribution
    except Exception as e:
        # Fallback to Integrated Gradients if LRP fails
        ig = IntegratedGradients(model)
        attribution = ig.attribute(image_tensor, target=target_class, n_steps=50)
        return attribution

def generate_integrated_gradients(model, image_tensor, target_class):
    """Generate Integrated Gradients"""
    ig = IntegratedGradients(model)
    attribution = ig.attribute(image_tensor, target=target_class, n_steps=50)
    
    # Add noise tunnel for more robust explanations
    nt = NoiseTunnel(ig)
    attribution_nt = nt.attribute(image_tensor, nt_type='smoothgrad', 
                                  nt_samples=10, target=target_class)
    return attribution, attribution_nt

def visualize_attribution(attribution, original_image, method_name, prediction_class):
    """Visualize attribution maps"""
    # Convert attribution to numpy
    if len(attribution.shape) == 4:
        attribution_np = attribution.squeeze().cpu().detach().numpy()
    else:
        attribution_np = attribution.cpu().detach().numpy()
    
    # For multi-channel attributions, take the mean across channels
    if len(attribution_np.shape) == 3:
        attribution_np = np.mean(attribution_np, axis=0)
    
    # Create visualization
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Original image
    if isinstance(original_image, torch.Tensor):
        img_np = original_image.squeeze().permute(1, 2, 0).cpu().numpy()
        # Denormalize
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_np = img_np * std + mean
        img_np = np.clip(img_np, 0, 1)
    else:
        img_np = np.array(original_image) / 255.0
    
    axes[0].imshow(img_np)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # Attribution heatmap
    im1 = axes[1].imshow(attribution_np, cmap='RdYlBu_r')
    axes[1].set_title(f'{method_name} Attribution')
    axes[1].axis('off')
    plt.colorbar(im1, ax=axes[1])
    
    # Overlay
    axes[2].imshow(img_np)
    axes[2].imshow(attribution_np, cmap='RdYlBu_r', alpha=0.6)
    axes[2].set_title(f'{method_name} Overlay')
    axes[2].axis('off')
    
    plt.suptitle(f'{method_name} Explanation for {prediction_class.upper()} Prediction')
    plt.tight_layout()
    
    return fig

# Fairness and Bias Detection with Threshold Optimization
class AdvancedBiasDetector:
    def __init__(self):
        self.post_processor = None
        self.base_estimator = None
        self.is_fitted = False
        
    def prepare_fairness_data(self, predictions_data):
        """Prepare data for fairness analysis"""
        if len(predictions_data) < 5:  # Reduced from 10 to 5
            return None, None, None, None
            
        df = pd.DataFrame(predictions_data)
        
        # Create multiple sensitive attribute options for better bias detection
        np.random.seed(42)  # For reproducible results
        
        # Method 1: Based on confidence quartiles (more variation than median)
        conf_q75 = df['confidence'].quantile(0.75)
        conf_q25 = df['confidence'].quantile(0.25)
        
        # Create 3 groups: low, medium, high confidence
        df['conf_group'] = 0  # Low confidence
        df.loc[df['confidence'] >= conf_q25, 'conf_group'] = 1  # Medium confidence  
        df.loc[df['confidence'] >= conf_q75, 'conf_group'] = 2  # High confidence
        
        # Use binary grouping: low+medium vs high confidence
        df['sensitive_attr'] = (df['conf_group'] >= 1).astype(int)
        
        # Alternative: Mix temporal and confidence patterns for more variation
        temporal_group = (df.index >= len(df) // 2).astype(int)
        confidence_group = (df['confidence'] > df['confidence'].median()).astype(int)
        
        # Combine both patterns for richer bias simulation
        df['sensitive_attr'] = ((temporal_group + confidence_group) >= 1).astype(int)
        
        # Ensure both groups have reasonable sizes
        group_0_size = np.sum(df['sensitive_attr'] == 0)
        group_1_size = np.sum(df['sensitive_attr'] == 1)
        
        # If groups are too unbalanced, rebalance them
        if group_0_size < len(df) * 0.2 or group_1_size < len(df) * 0.2:
            # Use alternating pattern for better balance
            df['sensitive_attr'] = (df.index % 2).astype(int)
        
        # Create binary outcomes - simulate some ground truth with REALISTIC bias
        y_pred_original = (df['prediction'] == 'malignant').astype(int)
        y_scores = df['confidence'].values
        
        # Create synthetic ground truth with more aggressive bias patterns
        y_true = y_pred_original.copy()
        
        # Get group masks
        group_0_mask = df['sensitive_attr'] == 0
        group_1_mask = df['sensitive_attr'] == 1
        
        # Ensure both groups have mixed labels to avoid degenerate labels error
        group_0_indices = np.where(group_0_mask)[0]
        group_1_indices = np.where(group_1_mask)[0]
        
        # Force label diversity in both groups
        if len(group_0_indices) > 0:
            # Ensure Group 0 has both positive and negative labels
            group_0_labels = y_true[group_0_indices]
            if np.all(group_0_labels == 0) or np.all(group_0_labels == 1):
                # Force diversity by flipping some labels
                flip_count = max(1, len(group_0_indices) // 3)
                flip_indices = np.random.choice(group_0_indices, size=flip_count, replace=False)
                y_true[flip_indices] = 1 - y_true[flip_indices]  # Flip labels
            
            # Additional bias: Higher false positive rate (tends to over-diagnose malignant)
            benign_in_group_0 = group_0_indices[y_true[group_0_indices] == 0]
            if len(benign_in_group_0) > 1:  # Need at least 2 to flip some
                flip_count = max(1, int(0.25 * len(benign_in_group_0)))
                flip_indices = np.random.choice(benign_in_group_0, size=flip_count, replace=False)
                y_true[flip_indices] = 1  # Flip benign to malignant (false positive)
        
        if len(group_1_indices) > 0:
            # Ensure Group 1 has both positive and negative labels
            group_1_labels = y_true[group_1_indices]
            if np.all(group_1_labels == 0) or np.all(group_1_labels == 1):
                # Force diversity by flipping some labels
                flip_count = max(1, len(group_1_indices) // 3)
                flip_indices = np.random.choice(group_1_indices, size=flip_count, replace=False)
                y_true[flip_indices] = 1 - y_true[flip_indices]  # Flip labels
            
            # Additional bias: Higher false negative rate (tends to miss malignant cases)  
            malignant_in_group_1 = group_1_indices[y_true[group_1_indices] == 1]
            if len(malignant_in_group_1) > 1:  # Need at least 2 to flip some
                flip_count = max(1, int(0.2 * len(malignant_in_group_1)))
                flip_indices = np.random.choice(malignant_in_group_1, size=flip_count, replace=False)
                y_true[flip_indices] = 0  # Flip malignant to benign (false negative)
        
        # Use dynamic threshold based on data distribution for binary predictions
        optimal_threshold = 0.5
        if len(y_scores) > 10:
            # Find threshold that maximizes accuracy
            thresholds = np.linspace(0.3, 0.7, 20)
            best_threshold = 0.5
            best_accuracy = 0
            
            for thresh in thresholds:
                y_pred_temp = (y_scores > thresh).astype(int)
                accuracy = np.mean(y_pred_temp == y_pred_original)
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_threshold = thresh
            
            optimal_threshold = best_threshold
        
        y_pred = (y_scores > optimal_threshold).astype(int)
        sensitive_features = df['sensitive_attr'].values
        
        return y_true, y_pred, y_scores, sensitive_features
    
    def fit_threshold_optimizer(self, y_true, y_scores, sensitive_features):
        """Fit Threshold Optimizer for equalized odds"""
        try:
            # Split data for post-processing
            if len(y_true) < 15:  # Reduced from 20 to match UI requirement
                return False
                
            # Create train/test split for post-processing
            indices = np.arange(len(y_true))
            train_idx, test_idx = train_test_split(indices, test_size=0.3, random_state=42)
            
            # Create a simple base estimator that returns the scores
            self.base_estimator = LogisticRegression()
            
            # Fit base estimator on scores (treating scores as features)
            X_train = y_scores[train_idx].reshape(-1, 1)
            y_train = y_true[train_idx]
            sensitive_train = sensitive_features[train_idx]
            
            self.base_estimator.fit(X_train, y_train)
            
            # Create threshold optimizer for equalized odds
            self.post_processor = ThresholdOptimizer(
                estimator=self.base_estimator,
                constraints='equalized_odds',
                objective='accuracy_score'
            )
            
            # Fit the threshold optimizer
            self.post_processor.fit(
                X_train, 
                y_train, 
                sensitive_features=sensitive_train
            )
            
            self.is_fitted = True
            return True
            
        except Exception as e:
            st.error(f"Error fitting Threshold Optimizer: {str(e)}")
            return False
    
    def predict_fair(self, y_scores, sensitive_features):
        """Make fair predictions using post-processing"""
        if not self.is_fitted:
            return None
            
        try:
            X_test = y_scores.reshape(-1, 1)
            fair_predictions = self.post_processor.predict(
                X_test,
                sensitive_features=sensitive_features
            )
            return fair_predictions
        except Exception as e:
            return None
    
    def calculate_fairness_metrics(self, y_true, y_pred, y_pred_fair, sensitive_features):
        """Calculate comprehensive fairness metrics"""
        metrics = {}
        
        try:
            # Convert to pandas Series for fairlearn metrics
            y_true_series = pd.Series(y_true)
            y_pred_series = pd.Series(y_pred)
            sensitive_series = pd.Series(sensitive_features)
            
            # Calculate equalized odds difference with error handling
            try:
                eq_odds_diff = equalized_odds_difference(
                    y_true_series, y_pred_series, sensitive_features=sensitive_series
                )
                metrics['equalized_odds_violation'] = abs(eq_odds_diff)
            except Exception as e:
                # Manual calculation if fairlearn fails
                metrics['equalized_odds_violation'] = self._manual_equalized_odds(y_true, y_pred, sensitive_features)
            
            # Calculate demographic parity difference with error handling
            try:
                demo_parity_diff = demographic_parity_difference(
                    y_true_series, y_pred_series, sensitive_features=sensitive_series
                )
                metrics['demographic_parity_violation'] = abs(demo_parity_diff)
            except Exception as e:
                # Manual calculation if fairlearn fails
                metrics['demographic_parity_violation'] = self._manual_demographic_parity(y_pred, sensitive_features)
            
            # Group-wise metrics for detailed analysis
            group_0_mask = sensitive_features == 0
            group_1_mask = sensitive_features == 1
            
            # Ensure both groups exist and have data
            if np.sum(group_0_mask) > 0 and np.sum(group_1_mask) > 0:
                # True Positive Rates - with better handling for edge cases
                tpr_0, tpr_1 = self._calculate_group_tpr(y_true, y_pred, group_0_mask, group_1_mask)
                metrics['tpr_difference'] = abs(tpr_0 - tpr_1)
                
                # False Positive Rates - with better handling for edge cases
                fpr_0, fpr_1 = self._calculate_group_fpr(y_true, y_pred, group_0_mask, group_1_mask)
                metrics['fpr_difference'] = abs(fpr_0 - fpr_1)
                
                # Store individual rates for debugging
                metrics['tpr_group_0'] = tpr_0
                metrics['tpr_group_1'] = tpr_1
                metrics['fpr_group_0'] = fpr_0
                metrics['fpr_group_1'] = fpr_1
                
                # Fair metrics if available
                if y_pred_fair is not None:
                    try:
                        y_pred_fair_series = pd.Series(y_pred_fair)
                        
                        # Fair equalized odds
                        eq_odds_diff_fair = equalized_odds_difference(
                            y_true_series, y_pred_fair_series, sensitive_features=sensitive_series
                        )
                        metrics['equalized_odds_violation_fair'] = abs(eq_odds_diff_fair)
                        
                        # Fair demographic parity
                        demo_parity_diff_fair = demographic_parity_difference(
                            y_true_series, y_pred_fair_series, sensitive_features=sensitive_series
                        )
                        metrics['demographic_parity_violation_fair'] = abs(demo_parity_diff_fair)
                        
                        # Fair TPR calculations
                        tpr_fair_0, tpr_fair_1 = self._calculate_group_tpr(y_true, y_pred_fair, group_0_mask, group_1_mask)
                        metrics['tpr_difference_fair'] = abs(tpr_fair_0 - tpr_fair_1)
                        
                    except Exception as e:
                        # If fair metrics fail, set to original values
                        metrics['equalized_odds_violation_fair'] = metrics['equalized_odds_violation']
                        metrics['demographic_parity_violation_fair'] = metrics['demographic_parity_violation']
                        metrics['tpr_difference_fair'] = metrics['tpr_difference']
            else:
                # If only one group exists, set differences to zero but note the issue
                metrics['equalized_odds_violation'] = 0.0
                metrics['demographic_parity_violation'] = 0.0
                metrics['tpr_difference'] = 0.0
                metrics['fpr_difference'] = 0.0
                metrics['single_group_warning'] = True
        
        except Exception as e:
            # Complete fallback to manual calculations
            metrics = self._manual_fairness_calculation(y_true, y_pred, sensitive_features)
        
        return metrics
    
    def _manual_equalized_odds(self, y_true, y_pred, sensitive_features):
        """Manual calculation of equalized odds difference"""
        try:
            group_0_mask = sensitive_features == 0
            group_1_mask = sensitive_features == 1
            
            if np.sum(group_0_mask) == 0 or np.sum(group_1_mask) == 0:
                return 0.0
            
            # TPR difference
            tpr_0, tpr_1 = self._calculate_group_tpr(y_true, y_pred, group_0_mask, group_1_mask)
            tpr_diff = abs(tpr_0 - tpr_1)
            
            # FPR difference  
            fpr_0, fpr_1 = self._calculate_group_fpr(y_true, y_pred, group_0_mask, group_1_mask)
            fpr_diff = abs(fpr_0 - fpr_1)
            
            # Equalized odds is max of TPR and FPR differences
            return max(tpr_diff, fpr_diff)
        except:
            return 0.0
    
    def _manual_demographic_parity(self, y_pred, sensitive_features):
        """Manual calculation of demographic parity difference"""
        try:
            group_0_mask = sensitive_features == 0
            group_1_mask = sensitive_features == 1
            
            if np.sum(group_0_mask) == 0 or np.sum(group_1_mask) == 0:
                return 0.0
            
            # Positive prediction rates
            pos_rate_0 = np.mean(y_pred[group_0_mask])
            pos_rate_1 = np.mean(y_pred[group_1_mask])
            
            return abs(pos_rate_0 - pos_rate_1)
        except:
            return 0.0
    
    def _calculate_group_tpr(self, y_true, y_pred, group_0_mask, group_1_mask):
        """Calculate True Positive Rates for both groups"""
        # Group 0 TPR
        group_0_positives = np.sum(y_true[group_0_mask] == 1)
        if group_0_positives > 0:
            tpr_0 = np.sum((y_true[group_0_mask] == 1) & (y_pred[group_0_mask] == 1)) / group_0_positives
        else:
            tpr_0 = 0.0
        
        # Group 1 TPR
        group_1_positives = np.sum(y_true[group_1_mask] == 1)
        if group_1_positives > 0:
            tpr_1 = np.sum((y_true[group_1_mask] == 1) & (y_pred[group_1_mask] == 1)) / group_1_positives
        else:
            tpr_1 = 0.0
        
        return tpr_0, tpr_1
    
    def _calculate_group_fpr(self, y_true, y_pred, group_0_mask, group_1_mask):
        """Calculate False Positive Rates for both groups"""
        # Group 0 FPR
        group_0_negatives = np.sum(y_true[group_0_mask] == 0)
        if group_0_negatives > 0:
            fpr_0 = np.sum((y_true[group_0_mask] == 0) & (y_pred[group_0_mask] == 1)) / group_0_negatives
        else:
            fpr_0 = 0.0
        
        # Group 1 FPR
        group_1_negatives = np.sum(y_true[group_1_mask] == 0)
        if group_1_negatives > 0:
            fpr_1 = np.sum((y_true[group_1_mask] == 0) & (y_pred[group_1_mask] == 1)) / group_1_negatives
        else:
            fpr_1 = 0.0
        
        return fpr_0, fpr_1
    
    def _manual_fairness_calculation(self, y_true, y_pred, sensitive_features):
        """Complete manual fairness calculation as fallback"""
        metrics = {}
        
        try:
            metrics['equalized_odds_violation'] = self._manual_equalized_odds(y_true, y_pred, sensitive_features)
            metrics['demographic_parity_violation'] = self._manual_demographic_parity(y_pred, sensitive_features)
            
            group_0_mask = sensitive_features == 0
            group_1_mask = sensitive_features == 1
            
            if np.sum(group_0_mask) > 0 and np.sum(group_1_mask) > 0:
                tpr_0, tpr_1 = self._calculate_group_tpr(y_true, y_pred, group_0_mask, group_1_mask)
                fpr_0, fpr_1 = self._calculate_group_fpr(y_true, y_pred, group_0_mask, group_1_mask)
                
                metrics['tpr_difference'] = abs(tpr_0 - tpr_1)
                metrics['fpr_difference'] = abs(fpr_0 - fpr_1)
                metrics['tpr_group_0'] = tpr_0
                metrics['tpr_group_1'] = tpr_1
                metrics['fpr_group_0'] = fpr_0
                metrics['fpr_group_1'] = fpr_1
            else:
                metrics['tpr_difference'] = 0.0
                metrics['fpr_difference'] = 0.0
                metrics['single_group_warning'] = True
        except:
            # Absolute fallback
            metrics['equalized_odds_violation'] = 0.1  # Set to small non-zero value
            metrics['demographic_parity_violation'] = 0.1
            metrics['tpr_difference'] = 0.1
            metrics['fpr_difference'] = 0.1
            metrics['calculation_error'] = True
        
        return metrics

# Predict function for LIME
def predict_for_lime(images, model, transform):
    """Prediction function that LIME can use"""
    batch_predictions = []
    
    for image in images:
        # Convert to PIL Image and apply transforms
        if image.max() <= 1.0:
            image = (image * 255).astype(np.uint8)
        
        pil_image = Image.fromarray(image)
        img_tensor = transform(pil_image).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(img_tensor)
            probabilities = torch.softmax(outputs, dim=1)[0].numpy()
            batch_predictions.append(probabilities)
    
    return np.array(batch_predictions)

# Predict
def predict(image, model):
    img_tensor = transform(image).unsqueeze(0)
    with torch.no_grad():
        outputs = model(img_tensor)
        _, preds = torch.max(outputs, 1)
        confidence = torch.softmax(outputs, dim=1)[0, preds.item()].item()
    return class_names[preds.item()], confidence, img_tensor

# Fairness Check
def fairness_check(conf):
    if conf < 0.6:
        return "âš ï¸ Low confidence â€” potential uncertainty"
    return "âœ… Confident prediction"

# Streamlit UI
st.set_page_config(
    page_title="Breast Cancer Predictor", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize model and blockchain
@st.cache_resource
def init_app():
    model = load_model()
    lime_explainer = load_lime_explainer()
    explainability_methods = load_explainability_methods()
    return model, lime_explainer, explainability_methods

model, lime_explainer, explainability_methods = init_app()

# Initialize bias detector in session state
if 'bias_detector' not in st.session_state:
    st.session_state.bias_detector = AdvancedBiasDetector()

# Initialize blockchain in session state for persistence
if 'blockchain' not in st.session_state:
    st.session_state.blockchain = BlockchainLogger()
blockchain = st.session_state.blockchain

# Sidebar Navigation
st.sidebar.title("ðŸ¥ Navigation")
page = st.sidebar.selectbox(
    "Choose a page:",
    ["ðŸ©º Main Diagnosis", "ðŸ” LIME Explanations", "ðŸ§  Advanced Explainability", "ðŸ”— Blockchain Data", "âš–ï¸ Advanced Bias Detection"]
)

# Page Functions
def main_diagnosis_page():
    st.title("ðŸ©º Breast Cancer Diagnosis with AI")
    st.subheader("Using ResNet18 + LIME + Custom Blockchain")
    
    # Upload image
    uploaded_file = st.file_uploader("Upload a breast cancer image (benign/malignant)", type=["jpg", "jpeg", "png"])

    if uploaded_file:
        image = Image.open(uploaded_file).convert("RGB")
        
        # Display image with smaller width
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.image(image, caption="Uploaded Image", width=400)

        # Predict
        pred_label, confidence, img_tensor = predict(image, model)

        # Show prediction
        st.success(f"### Prediction: {pred_label.upper()} with confidence {confidence:.2%}")
        st.info(fairness_check(confidence))

        # Log to blockchain with image data
        
        # Convert image to base64 for storage
        img_buffer = io.BytesIO()
        image.save(img_buffer, format='PNG')
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        
        blockchain.log_ai_metrics("ResNet18_BreastCancer", {
            'prediction': pred_label,
            'confidence': confidence,
            'filename': uploaded_file.name,
            'image_data': img_base64  # Store image for LIME page
        })

        # Quick LIME preview
        st.subheader("ðŸ” Quick Explanation Preview")
        st.info("ðŸ’¡ For detailed LIME analysis, visit the 'LIME Explanations' page from the sidebar")
        
        with st.expander("ðŸ§¾ Latest Blockchain Entry"):
            st.json(blockchain.chain[-1])

def lime_explanations_page():
    st.title("ðŸ” LIME Model Explainability")
    st.write("**Local Interpretable Model-agnostic Explanations**")
    
    # Check if there are recent predictions
    stats = blockchain.get_prediction_stats()
    if not stats:
        st.warning("âš ï¸ No predictions found. Please make a prediction on the Main Diagnosis page first.")
        return
    
    # Filter stats that have image data
    stats_with_images = [s for s in stats if 'image_data' in s and s['image_data']]
    
    if not stats_with_images:
        st.warning("âš ï¸ No images found in recent predictions. The image storage feature was recently added.")
        return
    
    # Let user select which prediction to explain
    st.subheader("Select Prediction to Explain:")
    prediction_options = [f"#{p['block_index']} - {p['filename']} ({p['prediction']} - {p['confidence']:.1%})" 
                         for p in stats_with_images[-10:]]
    
    if prediction_options:
        selected_idx = st.selectbox("Choose a recent prediction:", range(len(prediction_options)), 
                                   format_func=lambda x: prediction_options[x])
        
        selected_prediction = stats_with_images[-10:][selected_idx]
        
        # Display selected prediction info
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Prediction", selected_prediction['prediction'].upper())
        with col2:
            st.metric("Confidence", f"{selected_prediction['confidence']:.1%}")
        with col3:
            st.metric("Block #", selected_prediction['block_index'])
        
        # Get image data from blockchain
        
        # Find the full block data
        selected_block = None
        for block in blockchain.chain:
            if block.get('index') == selected_prediction['block_index']:
                selected_block = block
                break
        
        if selected_block and 'image_data' in selected_block['data']['metrics']:
            try:
                # Decode image from base64
                img_data = base64.b64decode(selected_block['data']['metrics']['image_data'])
                image = Image.open(io.BytesIO(img_data))
                
                # Show original image
                st.subheader("ðŸ“‹ Selected Image")
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(image, caption=f"File: {selected_prediction['filename']}", width=400)
                
                # Generate LIME explanation button
                if st.button("ðŸ” Generate LIME Explanation", type="primary"):
                    with st.spinner("Generating LIME explanation... This may take 30-60 seconds."):
                        try:
                            # Convert PIL image to numpy array for LIME
                            image_array = np.array(image)
                            
                            # Create prediction function wrapper
                            def predict_fn(images):
                                return predict_for_lime(images, model, transform)
                            
                            # Generate LIME explanation
                            explanation = lime_explainer.explain_instance(
                                image_array, 
                                predict_fn, 
                                top_labels=2,  # Ask for both classes
                                hide_color=0, 
                                num_samples=200,  # Increased for better stability
                                num_features=40   # Increased to capture more features
                            )
                            
                            # Get explanation for the predicted class
                            pred_class_idx = 0 if selected_prediction['prediction'] == 'benign' else 1
                            temp_img, mask = explanation.get_image_and_mask(
                                pred_class_idx, 
                                positive_only=True, 
                                num_features=10, 
                                hide_rest=False
                            )
                            
                            # Create visualization
                            st.subheader("ðŸŽ¯ LIME Explanation Results")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.write("**Original Image**")
                                fig1, ax1 = plt.subplots(figsize=(8, 8))
                                ax1.imshow(image_array)
                                ax1.set_title(f"Original Image\nPrediction: {selected_prediction['prediction'].upper()}")
                                ax1.axis('off')
                                st.pyplot(fig1)
                                plt.close(fig1)
                            
                            with col2:
                                st.write("**LIME Explanation**")
                                fig2, ax2 = plt.subplots(figsize=(8, 8))
                                ax2.imshow(mark_boundaries(temp_img, mask))
                                ax2.set_title(f"LIME Explanation\nImportant regions for '{selected_prediction['prediction']}'")
                                ax2.axis('off')
                                st.pyplot(fig2)
                                plt.close(fig2)
                            
                            # Show feature importance graph
                            st.subheader("ï¿½ Feature Importance Scores")
                            st.write("**Understanding the Graph:**")
                            st.write("- **X-axis**: Image segments (numbered regions)")
                            st.write("- **Y-axis**: Importance score (how much each segment influenced the prediction)")
                            st.write("- **Positive scores**: Support the predicted class")
                            st.write("- **Negative scores**: Work against the predicted class")
                            
                            try:
                                # Get feature importance data
                                features = explanation.local_exp[pred_class_idx]
                                if features:
                                    # Look at up to 30 features to find good opposing evidence
                                    feature_df = pd.DataFrame(features, columns=['Segment', 'Importance'])
                                    feature_df = feature_df.sort_values('Importance', key=abs, ascending=False).head(30)
                                    
                                    # Debug information about the features
                                    st.subheader("ðŸ” Debug Information")
                                    debug_col1, debug_col2, debug_col3 = st.columns(3)
                                    
                                    with debug_col1:
                                        positive_count = len(feature_df[feature_df['Importance'] > 0])
                                        st.metric("Positive Features", positive_count)
                                    
                                    with debug_col2:
                                        negative_count = len(feature_df[feature_df['Importance'] < 0])
                                        st.metric("Negative Features", negative_count)
                                    
                                    with debug_col3:
                                        total_range = feature_df['Importance'].max() - feature_df['Importance'].min()
                                        st.metric("Value Range", f"{total_range:.4f}")
                                    
                                    # Show raw data for debugging
                                    with st.expander("ðŸ“Š Raw Feature Data"):
                                        st.dataframe(feature_df.head(20))  # Show more raw data
                                    
                                    # Simple enhancement: if we don't have negative features, force some from opposite class
                                    if len(feature_df[feature_df['Importance'] < 0]) == 0:
                                        st.info("â„¹ï¸ No opposing features found in main prediction. Adding opposite class features...")
                                        opposite_class_idx = 1 - pred_class_idx
                                        opposite_features = explanation.local_exp.get(opposite_class_idx, [])
                                        
                                        if opposite_features:
                                            # Take top 10 from current class and top 10 from opposite (as negative)
                                            current_top = feature_df.head(10)  # Increased from 8
                                            opposite_df = pd.DataFrame(opposite_features, columns=['Segment', 'Opposite_Importance'])
                                            opposite_top = opposite_df.nlargest(10, 'Opposite_Importance')  # Increased from 7
                                            
                                            # Combine them
                                            mixed_data = []
                                            for _, row in current_top.iterrows():
                                                mixed_data.append([row['Segment'], row['Importance']])
                                            for _, row in opposite_top.iterrows():
                                                mixed_data.append([row['Segment'], -row['Opposite_Importance']])
                                            
                                            feature_df = pd.DataFrame(mixed_data, columns=['Segment', 'Importance'])
                                            feature_df = feature_df.sort_values('Importance', key=abs, ascending=False).head(20)  # Show top 20
                                            st.success("âœ… Mixed features from both classes for better visualization!")
                                    
                                    # Create the feature importance bar chart with better colors
                                    # Add color based on positive/negative values
                                    feature_df['Color'] = feature_df['Importance'].apply(
                                        lambda x: 'Positive (Supporting)' if x > 0 else 'Negative (Opposing)'
                                    )
                                    
                                    fig3 = px.bar(
                                        feature_df, 
                                        x='Segment', 
                                        y='Importance', 
                                        title=f"Top {len(feature_df)} Most Important Image Segments for '{selected_prediction['prediction'].upper()}' Prediction",
                                        color='Color',
                                        color_discrete_map={
                                            'Positive (Supporting)': '#2E8B57',  # Sea Green
                                            'Negative (Opposing)': '#DC143C'     # Crimson Red
                                        },
                                        labels={
                                            'Segment': 'Image Segment ID',
                                            'Importance': 'Importance Score',
                                            'Color': 'Influence Type'
                                        },
                                        hover_data={'Importance': ':.4f'}
                                    )
                                    
                                    # Update layout for better visibility
                                    fig3.update_layout(
                                        plot_bgcolor='#F8F9FA',  # Light gray background
                                        paper_bgcolor='white',
                                        height=500,
                                        title={
                                            'font': {'size': 16, 'color': 'black'},
                                            'x': 0.5,
                                            'xanchor': 'center'
                                        },
                                        xaxis={
                                            'title': {'font': {'color': 'black', 'size': 14}},
                                            'tickfont': {'color': 'black', 'size': 12},
                                            'gridcolor': 'lightgray',
                                            'showgrid': True
                                        },
                                        yaxis={
                                            'title': {'font': {'color': 'black', 'size': 14}},
                                            'tickfont': {'color': 'black', 'size': 12},
                                            'gridcolor': 'lightgray',
                                            'showgrid': True,
                                            'zeroline': True,
                                            'zerolinecolor': 'black',
                                            'zerolinewidth': 2
                                        },
                                        legend={
                                            'font': {'color': 'black', 'size': 12},
                                            'bgcolor': 'rgba(255,255,255,0.8)',
                                            'bordercolor': 'black',
                                            'borderwidth': 1
                                        }
                                    )
                                    
                                    # Add hover template for better interactivity
                                    fig3.update_traces(
                                        hovertemplate='<b>Segment %{x}</b><br>' +
                                                     'Importance: %{y:.4f}<br>' +
                                                     '%{fullData.name}<br>' +
                                                     '<extra></extra>'
                                    )
                                    
                                    st.plotly_chart(fig3, use_container_width=True)
                                    
                                    # Show detailed explanation
                                    st.subheader("ðŸ“Š Detailed Feature Analysis")
                                    
                                    # Split features into positive and negative (show more features)
                                    positive_features = feature_df[feature_df['Importance'] > 0].head(8)  # Increased from 5
                                    negative_features = feature_df[feature_df['Importance'] < 0].head(8)  # Increased from 5
                                    
                                    col_pos, col_neg = st.columns(2)
                                    
                                    with col_pos:
                                        st.write("**ðŸŸ¢ Segments Supporting the Prediction:**")
                                        if not positive_features.empty:
                                            for _, row in positive_features.iterrows():
                                                st.write(f"â€¢ Segment {int(row['Segment'])}: {row['Importance']:.3f}")
                                        else:
                                            st.write("No strongly supporting segments found")
                                    
                                    with col_neg:
                                        st.write("**ðŸ”´ Segments Working Against the Prediction:**")
                                        if not negative_features.empty:
                                            for _, row in negative_features.iterrows():
                                                st.write(f"â€¢ Segment {int(row['Segment'])}: {row['Importance']:.3f}")
                                        else:
                                            st.write("No strongly opposing segments found")
                                    
                                    # Summary statistics
                                    st.subheader("ðŸ“‹ Summary Statistics")
                                    total_positive = feature_df[feature_df['Importance'] > 0]['Importance'].sum()
                                    total_negative = abs(feature_df[feature_df['Importance'] < 0]['Importance'].sum())
                                    
                                    summary_col1, summary_col2, summary_col3 = st.columns(3)
                                    with summary_col1:
                                        st.metric("Total Supporting Evidence", f"{total_positive:.3f}")
                                    with summary_col2:
                                        st.metric("Total Opposing Evidence", f"{total_negative:.3f}")
                                    with summary_col3:
                                        net_support = total_positive - total_negative
                                        st.metric("Net Support", f"{net_support:.3f}")
                                    
                                else:
                                    st.warning("âš ï¸ No feature importance data available for this prediction")
                                    
                            except Exception as e:
                                st.warning(f"âš ï¸ Could not generate feature importance graph: {str(e)}")
                                st.info("This can happen with certain image types. The LIME explanation above is still valid.")
                            
                            # Show explanation details
                            st.subheader("ðŸ“– How to Interpret These Results")
                            
                            interpretation_col1, interpretation_col2 = st.columns(2)
                            
                            with interpretation_col1:
                                st.write("**ðŸ” Visual Elements:**")
                                st.write("- **Green boundaries**: Mark the most important regions")
                                st.write("- **Highlighted areas**: Features the AI focused on")
                                st.write("- **Segment numbers**: Correspond to the graph above")
                                st.write("- **Colors**: Intensity shows importance level")
                            
                            with interpretation_col2:
                                st.write("**ðŸ“Š Graph Elements:**")
                                st.write("- **High positive bars**: Strong evidence FOR the prediction")
                                st.write("- **High negative bars**: Strong evidence AGAINST the prediction")
                                st.write("- **Segment IDs**: Match the highlighted regions in the image")
                                st.write("- **Color coding**: Green = positive, Red = negative influence")
                            
                            st.success("âœ… LIME explanation completed successfully!")
                            
                        except Exception as e:
                            st.error(f"âŒ Error generating LIME explanation: {str(e)}")
                            st.info("This can happen with certain image types. Try with a different image.")
                
            except Exception as e:
                st.error(f"âŒ Error loading image: {str(e)}")
        else:
            st.error("âŒ Image data not found in blockchain record")
    
    # LIME Information Section
    st.markdown("---")
    st.subheader("â„¹ï¸ About LIME (Local Interpretable Model-agnostic Explanations)")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write("**How LIME Works:**")
        st.write("1. **Segmentation**: Divides image into meaningful regions")
        st.write("2. **Perturbation**: Creates thousands of variations by hiding regions")
        st.write("3. **Prediction**: Tests AI model on each variation")
        st.write("4. **Analysis**: Identifies which regions most affect the prediction")
        st.write("5. **Visualization**: Highlights important areas with green boundaries")
    
    with col2:
        st.write("**Medical Benefits:**")
        st.write("- ðŸ©º **Validation**: Verify AI is looking at medically relevant areas")
        st.write("- ðŸŽ“ **Education**: Learn what features indicate benign vs malignant")
        st.write("- ðŸ” **Quality Control**: Catch if AI focuses on irrelevant artifacts")
        st.write("- ðŸ¤ **Trust**: Build confidence in AI-assisted diagnosis")
        st.write("- ðŸ“‹ **Documentation**: Visual evidence for medical records")

def advanced_explainability_page():
    st.title("ðŸ§  Advanced Explainability Dashboard")
    st.write("**Class Activation Mapping, Saliency Maps & Layer-wise Relevance Propagation**")
    
    # Check if there are recent predictions
    stats = blockchain.get_prediction_stats()
    if not stats:
        st.warning("âš ï¸ No predictions found. Please make a prediction on the Main Diagnosis page first.")
        return
    
    # Filter stats that have image data
    stats_with_images = [s for s in stats if 'image_data' in s and s['image_data']]
    
    if not stats_with_images:
        st.warning("âš ï¸ No images found in recent predictions.")
        return
    
    # Let user select which prediction to explain
    st.subheader("Select Prediction to Analyze:")
    prediction_options = [f"#{p['block_index']} - {p['filename']} ({p['prediction']} - {p['confidence']:.1%})" 
                         for p in stats_with_images[-10:]]
    
    if prediction_options:
        selected_idx = st.selectbox("Choose a recent prediction:", range(len(prediction_options)), 
                                   format_func=lambda x: prediction_options[x])
        
        selected_prediction = stats_with_images[-10:][selected_idx]
        
        # Display selected prediction info
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Prediction", selected_prediction['prediction'].upper())
        with col2:
            st.metric("Confidence", f"{selected_prediction['confidence']:.1%}")
        with col3:
            st.metric("Block #", selected_prediction['block_index'])
        
        # Get image data from blockchain
        
        # Find the full block data
        selected_block = None
        for block in blockchain.chain:
            if block.get('index') == selected_prediction['block_index']:
                selected_block = block
                break
        
        if selected_block and 'image_data' in selected_block['data']['metrics']:
            try:
                # Decode image from base64
                img_data = base64.b64decode(selected_block['data']['metrics']['image_data'])
                image = Image.open(io.BytesIO(img_data))
                
                # Show original image
                st.subheader("ðŸ“‹ Selected Image")
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(image, caption=f"File: {selected_prediction['filename']}", width=400)
                
                # Prepare image tensor
                image_tensor = transform(image).unsqueeze(0)
                target_class = 0 if selected_prediction['prediction'] == 'benign' else 1
                
                # Select explainability methods
                st.subheader("ðŸ”¬ Choose Explainability Methods:")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    use_saliency = st.checkbox("ðŸŽ¯ Saliency Maps", value=True)
                with col2:
                    use_gradcam = st.checkbox("ðŸ”¥ GradCAM", value=True)
                with col3:
                    use_lrp = st.checkbox("ðŸ”„ Layer LRP", value=True)
                with col4:
                    use_ig = st.checkbox("ðŸ“ˆ Integrated Gradients", value=True)
                
                if st.button("ðŸ§  Generate Advanced Explanations", type="primary"):
                    with st.spinner("Generating advanced explanations... This may take 1-2 minutes."):
                        try:
                            model.eval()
                            
                            # Container for results
                            explanations = {}
                            
                            # 1. Saliency Maps
                            if use_saliency:
                                with st.status("Generating Saliency Maps...") as status:
                                    saliency_attr = generate_saliency_map(model, image_tensor, target_class)
                                    explanations['saliency'] = saliency_attr
                                    status.update(label="âœ… Saliency Maps Complete", state="complete")
                            
                            # 2. GradCAM
                            if use_gradcam:
                                with st.status("Generating GradCAM...") as status:
                                    gradcam_attr = generate_gradcam(model, image_tensor, target_class)
                                    explanations['gradcam'] = gradcam_attr
                                    status.update(label="âœ… GradCAM Complete", state="complete")
                            
                            # 3. Layer-wise Relevance Propagation
                            if use_lrp:
                                with st.status("Generating Layer LRP...") as status:
                                    lrp_attr = generate_lrp(model, image_tensor, target_class)
                                    explanations['lrp'] = lrp_attr
                                    status.update(label="âœ… Layer LRP Complete", state="complete")
                            
                            # 4. Integrated Gradients
                            if use_ig:
                                with st.status("Generating Integrated Gradients...") as status:
                                    ig_attr, ig_nt_attr = generate_integrated_gradients(model, image_tensor, target_class)
                                    explanations['ig'] = ig_attr
                                    explanations['ig_nt'] = ig_nt_attr
                                    status.update(label="âœ… Integrated Gradients Complete", state="complete")
                            
                            # Display Results
                            st.subheader("ðŸŽ¯ Advanced Explainability Results")
                            
                            # Create tabs for different methods
                            tab_names = []
                            if 'saliency' in explanations:
                                tab_names.append("ðŸŽ¯ Saliency")
                            if 'gradcam' in explanations:
                                tab_names.append("ðŸ”¥ GradCAM")
                            if 'lrp' in explanations:
                                tab_names.append("ðŸ”„ Layer LRP")
                            if 'ig' in explanations:
                                tab_names.append("ðŸ“ˆ Integrated Gradients")
                            if 'ig_nt' in explanations:
                                tab_names.append("ðŸŒŠ SmoothGrad IG")
                            
                            tabs = st.tabs(tab_names)
                            
                            tab_idx = 0
                            
                            # Saliency Maps
                            if 'saliency' in explanations:
                                with tabs[tab_idx]:
                                    st.write("**Saliency Maps** show pixel-level importance for the prediction")
                                    fig = visualize_attribution(explanations['saliency'], image_tensor, 
                                                               "Saliency", selected_prediction['prediction'])
                                    st.pyplot(fig)
                                    
                                    st.info("ðŸ’¡ **Interpretation**: Bright areas indicate pixels that most influence the model's decision. " +
                                           "Red areas increase confidence, blue areas decrease it.")
                                tab_idx += 1
                            
                            # GradCAM
                            if 'gradcam' in explanations:
                                with tabs[tab_idx]:
                                    st.write("**GradCAM** highlights important regions using convolutional layer activations")
                                    fig = visualize_attribution(explanations['gradcam'], image_tensor, 
                                                               "GradCAM", selected_prediction['prediction'])
                                    st.pyplot(fig)
                                    
                                    st.info("ðŸ’¡ **Interpretation**: GradCAM shows which regions the convolutional layers " +
                                           "focus on. Warmer colors indicate higher importance for the prediction.")
                                tab_idx += 1
                            
                            # Layer LRP
                            if 'lrp' in explanations:
                                with tabs[tab_idx]:
                                    st.write("**Layer-wise Relevance Propagation** decomposes predictions layer by layer")
                                    fig = visualize_attribution(explanations['lrp'], image_tensor, 
                                                               "Layer LRP", selected_prediction['prediction'])
                                    st.pyplot(fig)
                                    
                                    st.info("ðŸ’¡ **Interpretation**: LRP distributes the prediction score backwards through " +
                                           "the network, showing how each layer contributes to the final decision.")
                                tab_idx += 1
                            
                            # Integrated Gradients
                            if 'ig' in explanations:
                                with tabs[tab_idx]:
                                    st.write("**Integrated Gradients** provide stable attribution by integrating gradients")
                                    fig = visualize_attribution(explanations['ig'], image_tensor, 
                                                               "Integrated Gradients", selected_prediction['prediction'])
                                    st.pyplot(fig)
                                    
                                    st.info("ðŸ’¡ **Interpretation**: IG integrates gradients along a path from baseline to input, " +
                                           "providing more stable and accurate attributions than simple gradients.")
                                tab_idx += 1
                            
                            # SmoothGrad IG
                            if 'ig_nt' in explanations:
                                with tabs[tab_idx]:
                                    st.write("**SmoothGrad Integrated Gradients** reduce noise through sampling")
                                    fig = visualize_attribution(explanations['ig_nt'], image_tensor, 
                                                               "SmoothGrad IG", selected_prediction['prediction'])
                                    st.pyplot(fig)
                                    
                                    st.info("ðŸ’¡ **Interpretation**: SmoothGrad adds noise and averages results, " +
                                           "reducing visual noise and providing cleaner attribution maps.")
                            
                            # Comparison Summary
                            st.subheader("ðŸ“Š Method Comparison Summary")
                            
                            comparison_data = []
                            if 'saliency' in explanations:
                                comparison_data.append({
                                    'Method': 'Saliency Maps',
                                    'Speed': 'âš¡âš¡âš¡',
                                    'Accuracy': 'â­â­',
                                    'Interpretability': 'â­â­â­',
                                    'Best For': 'Quick pixel-level analysis'
                                })
                            
                            if 'gradcam' in explanations:
                                comparison_data.append({
                                    'Method': 'GradCAM',
                                    'Speed': 'âš¡âš¡',
                                    'Accuracy': 'â­â­â­',
                                    'Interpretability': 'â­â­â­â­',
                                    'Best For': 'Regional importance visualization'
                                })
                            
                            if 'lrp' in explanations:
                                comparison_data.append({
                                    'Method': 'Layer LRP',
                                    'Speed': 'âš¡',
                                    'Accuracy': 'â­â­â­â­',
                                    'Interpretability': 'â­â­â­â­â­',
                                    'Best For': 'Detailed layer-wise analysis'
                                })
                            
                            if 'ig' in explanations:
                                comparison_data.append({
                                    'Method': 'Integrated Gradients',
                                    'Speed': 'âš¡',
                                    'Accuracy': 'â­â­â­â­â­',
                                    'Interpretability': 'â­â­â­â­',
                                    'Best For': 'Stable, reliable attributions'
                                })
                            
                            if comparison_data:
                                comparison_df = pd.DataFrame(comparison_data)
                                st.dataframe(comparison_df, use_container_width=True)
                            
                        except Exception as e:
                            st.error(f"âŒ Error generating explanations: {str(e)}")
                            st.info("This can happen with certain images or methods. Try selecting fewer methods or a different image.")
                
            except Exception as e:
                st.error(f"âŒ Error loading image: {str(e)}")
        else:
            st.error("âŒ Image data not found in blockchain record")
    
    # Method Information Section
    st.markdown("---")
    st.subheader("â„¹ï¸ About Advanced Explainability Methods")
    
    method_info = {
        "ðŸŽ¯ Saliency Maps": {
            "description": "Compute gradients of the output with respect to input pixels",
            "advantages": ["Fast computation", "Pixel-level precision", "Simple to understand"],
            "limitations": ["Can be noisy", "May focus on irrelevant details", "Gradient saturation issues"]
        },
        "ðŸ”¥ GradCAM": {
            "description": "Use convolutional layer gradients to highlight important regions",
            "advantages": ["Region-level explanations", "Works with any CNN", "Visually intuitive"],
            "limitations": ["Lower resolution", "Depends on layer choice", "May miss fine details"]
        },
        "ðŸ”„ Layer LRP": {
            "description": "Decompose predictions by propagating relevance backward through layers",
            "advantages": ["Theoretically grounded", "Layer-wise insights", "Conservative attribution"],
            "limitations": ["Computationally intensive", "Requires specific rules", "Complex implementation"]
        },
        "ðŸ“ˆ Integrated Gradients": {
            "description": "Integrate gradients along path from baseline to input",
            "advantages": ["Mathematically principled", "Stable attributions", "Baseline comparison"],
            "limitations": ["Slower computation", "Baseline selection matters", "Path dependency"]
        }
    }
    
    for method, info in method_info.items():
        with st.expander(f"ðŸ“š {method}"):
            st.write(f"**Description**: {info['description']}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("**âœ… Advantages:**")
                for adv in info['advantages']:
                    st.write(f"â€¢ {adv}")
            
            with col2:
                st.write("**âš ï¸ Limitations:**")
                for lim in info['limitations']:
                    st.write(f"â€¢ {lim}")

def blockchain_data_page():
    st.title("ðŸ”— Blockchain Audit Trail")
    st.write("**Complete history of all AI predictions**")
    
    stats = blockchain.get_prediction_stats()
    
    if not stats:
        st.info("ðŸ“ No predictions recorded yet. Make some predictions to see the audit trail!")
        return
    
    # Create DataFrame
    df = pd.DataFrame(stats)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Predictions", len(df))
    with col2:
        benign_count = len(df[df['prediction'] == 'benign'])
        st.metric("Benign Cases", benign_count)
    with col3:
        malignant_count = len(df[df['prediction'] == 'malignant'])
        st.metric("Malignant Cases", malignant_count)
    with col4:
        avg_confidence = df['confidence'].mean()
        st.metric("Avg Confidence", f"{avg_confidence:.1%}")
    
    # Detailed table
    st.subheader("ðŸ“‹ Detailed Prediction Log")
    st.dataframe(df.sort_values('timestamp', ascending=False), use_container_width=True)
    
    # Download options
    st.subheader("ðŸ“¥ Export Data")
    col1, col2 = st.columns(2)
    with col1:
        csv = df.to_csv(index=False)
        st.download_button("ðŸ“Š Download CSV", csv, "predictions.csv", "text/csv")
    with col2:
        blockchain_json = json.dumps(blockchain.chain, indent=2)
        st.download_button("ðŸ”— Download Full Blockchain", blockchain_json, 
                          f"blockchain_{len(blockchain.chain)}_blocks.json", "application/json")

def advanced_bias_detection_page():
    st.title("âš–ï¸ Advanced Bias Detection with Threshold Optimization")
    st.write("**Monitor AI fairness using advanced post-processing techniques**")
    
    stats = blockchain.get_prediction_stats()
    bias_detector = st.session_state.bias_detector
    
    if len(stats) < 5:  # Reduced from 10 to 5
        st.warning(f"âš ï¸ Need at least 5 predictions for bias analysis. Current count: {len(stats)}")
        st.info("ðŸ’¡ Make more predictions to enable bias detection analysis.")
        
        # Show basic info even without enough data
        if len(stats) > 0:
            df = pd.DataFrame(stats)
            st.subheader("ðŸ“Š Current Prediction Summary")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Predictions", len(stats))
            with col2:
                benign_count = len([s for s in stats if s['prediction'] == 'benign'])
                st.metric("Benign", benign_count)
            with col3:
                malignant_count = len([s for s in stats if s['prediction'] == 'malignant'])
                st.metric("Malignant", malignant_count)
        return
    
    df = pd.DataFrame(stats)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Show basic statistics first
    st.subheader("ðŸ“Š Prediction Overview")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Predictions", len(stats))
    with col2:
        benign_count = len([s for s in stats if s['prediction'] == 'benign'])
        st.metric("Benign", benign_count)
    with col3:
        malignant_count = len([s for s in stats if s['prediction'] == 'malignant'])
        st.metric("Malignant", malignant_count)
    with col4:
        avg_conf = np.mean([s['confidence'] for s in stats])
        st.metric("Avg Confidence", f"{avg_conf:.1%}")
    
    # Prepare fairness data
    y_true, y_pred, y_scores, sensitive_features = bias_detector.prepare_fairness_data(stats)
    
    if y_true is None:
        st.error("âŒ Unable to prepare fairness analysis data")
        return
    
    # Add debugging section to understand why you're seeing zeros
    st.subheader("ðŸ” Debugging Information")
    
    debug_col1, debug_col2, debug_col3 = st.columns(3)
    
    with debug_col1:
        st.write("**Data Summary:**")
        st.write(f"â€¢ Predictions processed: {len(stats)}")
        st.write(f"â€¢ Group 0 size: {np.sum(sensitive_features == 0)}")
        st.write(f"â€¢ Group 1 size: {np.sum(sensitive_features == 1)}")
        st.write(f"â€¢ Confidence range: {min([s['confidence'] for s in stats]):.3f} - {max([s['confidence'] for s in stats]):.3f}")
    
    with debug_col2:
        st.write("**Prediction Breakdown:**")
        pred_types = [s['prediction'] for s in stats]
        benign_count = pred_types.count('benign')
        malignant_count = pred_types.count('malignant')
        st.write(f"â€¢ Benign predictions: {benign_count}")
        st.write(f"â€¢ Malignant predictions: {malignant_count}")
        st.write(f"â€¢ Y_true positives: {np.sum(y_true == 1)}")
        st.write(f"â€¢ Y_pred positives: {np.sum(y_pred == 1)}")
    
    with debug_col3:
        st.write("**Group Differences:**")
        group_0_mask = sensitive_features == 0
        group_1_mask = sensitive_features == 1
        
        if np.sum(group_0_mask) > 0 and np.sum(group_1_mask) > 0:
            # Show actual rates that are being compared
            tpr_0, tpr_1 = bias_detector._calculate_group_tpr(y_true, y_pred, group_0_mask, group_1_mask)
            fpr_0, fpr_1 = bias_detector._calculate_group_fpr(y_true, y_pred, group_0_mask, group_1_mask)
            
            st.write(f"â€¢ Group 0 TPR: {tpr_0:.3f}")
            st.write(f"â€¢ Group 1 TPR: {tpr_1:.3f}")
            st.write(f"â€¢ Group 0 FPR: {fpr_0:.3f}")
            st.write(f"â€¢ Group 1 FPR: {fpr_1:.3f}")
        else:
            st.write("â€¢ Only one group detected!")
    
    # Show confidence distribution details
    if st.expander("ðŸ“Š Detailed Confidence Analysis"):
        conf_values = [s['confidence'] for s in stats]
        st.write(f"**Confidence Statistics:**")
        st.write(f"â€¢ Mean: {np.mean(conf_values):.3f}")
        st.write(f"â€¢ Median: {np.median(conf_values):.3f}")
        st.write(f"â€¢ Std Dev: {np.std(conf_values):.3f}")
        st.write(f"â€¢ Min: {np.min(conf_values):.3f}")
        st.write(f"â€¢ Max: {np.max(conf_values):.3f}")
        
        # Show how groups were assigned
        group_assignment = pd.DataFrame({
            'Index': range(len(stats)),
            'Confidence': conf_values,
            'Prediction': [s['prediction'] for s in stats],
            'Group': sensitive_features
        })
        st.dataframe(group_assignment, use_container_width=True)
    
    # Show data distribution
    st.subheader("ðŸ“ˆ Data Distribution Analysis")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Confidence Distribution by Group**")
        group_0_conf = [stats[i]['confidence'] for i in range(len(stats)) if sensitive_features[i] == 0]
        group_1_conf = [stats[i]['confidence'] for i in range(len(stats)) if sensitive_features[i] == 1]
        
        fig = go.Figure()
        fig.add_trace(go.Box(y=group_0_conf, name="Group 0 (Low Conf)", boxpoints="all"))
        fig.add_trace(go.Box(y=group_1_conf, name="Group 1 (High Conf)", boxpoints="all"))
        fig.update_layout(title="Confidence by Sensitive Group", yaxis_title="Confidence")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.write("**Prediction Distribution**")
        pred_dist = pd.DataFrame({
            'Group': ['Group 0'] * len(group_0_conf) + ['Group 1'] * len(group_1_conf),
            'Prediction': [stats[i]['prediction'] for i in range(len(stats)) if sensitive_features[i] == 0] + 
                         [stats[i]['prediction'] for i in range(len(stats)) if sensitive_features[i] == 1]
        })
        
        if not pred_dist.empty:
            pred_counts = pred_dist.groupby(['Group', 'Prediction']).size().reset_index(name='Count')
            fig2 = px.bar(pred_counts, x='Group', y='Count', color='Prediction', 
                         title="Predictions by Group", barmode='group')
            st.plotly_chart(fig2, use_container_width=True)
    
    # Main Dashboard
    st.subheader("ðŸŽ¯ Fairness Analysis Dashboard")
    
    # Calculate fairness metrics
    fairness_metrics = bias_detector.calculate_fairness_metrics(y_true, y_pred, None, sensitive_features)
    
    # Add debugging for metrics calculation
    st.write("**ðŸ”§ Metrics Calculation Debug:**")
    debug_info = []
    for key, value in fairness_metrics.items():
        if not key.endswith('_group_0') and not key.endswith('_group_1'):
            debug_info.append(f"â€¢ {key}: {value}")
    
    if debug_info:
        for info in debug_info[:6]:  # Show first 6 metrics
            st.write(info)
    
    # Show if there are any special conditions
    if fairness_metrics.get('single_group_warning'):
        st.warning("âš ï¸ Only one group detected - bias metrics may be unreliable")
    if fairness_metrics.get('calculation_error'):
        st.error("âŒ Error in metrics calculation - using fallback values")
    
    # Display Key Metrics with more context
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        eq_odds_violation = fairness_metrics.get('equalized_odds_violation', 0)
        st.metric(
            "Equalized Odds Violation",
            f"{eq_odds_violation:.3f}",
            delta=f"{'Good' if eq_odds_violation < 0.1 else 'Needs Attention'} fairness",
            delta_color="normal" if eq_odds_violation < 0.1 else "inverse"
        )
        if eq_odds_violation == 0:
            st.caption("âš ï¸ May indicate insufficient data variation")
    
    with col2:
        demo_parity = fairness_metrics.get('demographic_parity_violation', 0)
        st.metric(
            "Demographic Parity Violation",
            f"{demo_parity:.3f}",
            delta=f"{'Good' if demo_parity < 0.1 else 'Needs Attention'} parity",
            delta_color="normal" if demo_parity < 0.1 else "inverse"
        )
        if demo_parity == 0:
            st.caption("âš ï¸ May indicate insufficient data variation")
    
    with col3:
        tpr_diff = fairness_metrics.get('tpr_difference', 0)
        st.metric(
            "TPR Difference",
            f"{tpr_diff:.3f}",
            delta="Between groups",
            delta_color="normal" if tpr_diff < 0.1 else "inverse"
        )
        if tpr_diff == 0:
            st.caption("âš ï¸ May need more diverse predictions")
    
    with col4:
        fpr_diff = fairness_metrics.get('fpr_difference', 0)
        st.metric(
            "FPR Difference", 
            f"{fpr_diff:.3f}",
            delta="Between groups",
            delta_color="normal" if fpr_diff < 0.1 else "inverse"
        )
        if fpr_diff == 0:
            st.caption("âš ï¸ May need more diverse predictions")
    
    # Show detailed group statistics
    st.subheader("ï¿½ Detailed Group Analysis")
    
    group_stats = []
    for group in [0, 1]:
        mask = sensitive_features == group
        group_name = f"Group {group} ({'Low Conf' if group == 0 else 'High Conf'})"
        
        if np.sum(mask) > 0:
            group_y_true = y_true[mask]
            group_y_pred = y_pred[mask]
            
            # Calculate basic metrics
            tp = np.sum((group_y_true == 1) & (group_y_pred == 1))
            tn = np.sum((group_y_true == 0) & (group_y_pred == 0))
            fp = np.sum((group_y_true == 0) & (group_y_pred == 1))
            fn = np.sum((group_y_true == 1) & (group_y_pred == 0))
            
            accuracy = (tp + tn) / len(group_y_true) if len(group_y_true) > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            
            group_stats.append({
                'Group': group_name,
                'Size': int(np.sum(mask)),
                'Accuracy': f"{accuracy:.3f}",
                'Precision': f"{precision:.3f}",
                'Recall': f"{recall:.3f}",
                'True Pos': int(tp),
                'True Neg': int(tn),
                'False Pos': int(fp),
                'False Neg': int(fn)
            })
    
    if group_stats:
        group_df = pd.DataFrame(group_stats)
        st.dataframe(group_df, use_container_width=True)
    
    # Threshold optimization section
    st.subheader("ðŸ”§ Advanced Threshold Optimization")
    
    if len(stats) >= 15:  # Reduced from 20 to 15
        if not bias_detector.is_fitted:
            if st.button("ðŸš€ Train Threshold Optimizer", type="primary"):
                with st.spinner("Training Threshold Optimizer for equalized odds..."):
                    success = bias_detector.fit_threshold_optimizer(y_true, y_scores, sensitive_features)
                    if success:
                        st.success("âœ… Threshold Optimizer trained successfully!")
                        st.rerun()
                    else:
                        st.error("âŒ Could not train post-processor. Need more diverse data.")
        else:
            st.success("âœ… Threshold Optimizer is trained and ready!")
            
            # Generate fair predictions
            y_pred_fair = bias_detector.predict_fair(y_scores, sensitive_features)
            
            if y_pred_fair is not None:
                # Show before/after comparison
                fair_metrics = bias_detector.calculate_fairness_metrics(y_true, y_pred, y_pred_fair, sensitive_features)
                
                st.write("**ðŸ“Š Before vs After Threshold Optimization:**")
                comparison_data = {
                    'Metric': ['Equalized Odds Violation', 'Demographic Parity Violation'],
                    'Before': [
                        f"{fairness_metrics.get('equalized_odds_violation', 0):.3f}",
                        f"{fairness_metrics.get('demographic_parity_violation', 0):.3f}"
                    ],
                    'After': [
                        f"{fair_metrics.get('equalized_odds_violation_fair', 0):.3f}",
                        f"{fair_metrics.get('demographic_parity_violation_fair', 0):.3f}"
                    ]
                }
                comparison_df = pd.DataFrame(comparison_data)
                st.dataframe(comparison_df, use_container_width=True)
    else:
        st.info(f"ðŸ”„ Need {15 - len(stats)} more predictions to enable Threshold Optimization training.")
    
    # Technical Details
    st.subheader("ðŸ”¬ Technical Details")
    
    with st.expander("ðŸ“š About This Bias Detection Implementation"):
        st.write("""
        **Current Implementation Details:**
        
        **Synthetic Data Generation:**
        - Groups are created based on confidence levels (high vs low confidence predictions)
        - Ground truth is simulated with bias patterns for demonstration
        - In production, you would use real demographic data and actual labels
        
        **Metrics Calculated:**
        - **Equalized Odds**: Measures if True Positive Rate and False Positive Rate are equal across groups
        - **Demographic Parity**: Measures if positive prediction rates are equal across groups
        - **TPR/FPR Differences**: Direct measurements of rate differences between groups
        
        **Why You Might See Zeros:**
        - Not enough data variation between groups
        - All predictions have similar confidence levels
        - Limited prediction diversity (all benign or all malignant)
        - Small sample sizes leading to identical group statistics
        
        **To See More Variation:**
        - Upload images with diverse confidence levels
        - Make predictions for both benign and malignant cases
        - Ensure at least 10-15 predictions with varied results
        """)
    
    # Recommendations
    st.subheader("ðŸ’¡ Fairness Recommendations")
    
    recommendations = []
    
    if len(stats) < 10:
        recommendations.append("ðŸ“ˆ **Collect More Data**: Upload more diverse images to enable robust bias analysis")
    
    if fairness_metrics.get('equalized_odds_violation', 0) > 0.1:
        recommendations.append("ðŸ”§ **High Equalized Odds Violation**: Consider using the Threshold Optimizer post-processor")
    elif fairness_metrics.get('equalized_odds_violation', 0) == 0:
        recommendations.append("ðŸ” **Zero Violation Detected**: This may indicate insufficient data variation. Try more diverse predictions.")
    
    if fairness_metrics.get('demographic_parity_violation', 0) > 0.1:
        recommendations.append("âš–ï¸ **Demographic Disparity**: Monitor for systematic bias in prediction rates")
    elif fairness_metrics.get('demographic_parity_violation', 0) == 0:
        recommendations.append("ðŸ“Š **No Parity Issues**: Either fair predictions or need more data variation")
    
    # Check for data diversity
    unique_predictions = len(set([s['prediction'] for s in stats]))
    conf_std = np.std([s['confidence'] for s in stats])
    
    if unique_predictions == 1:
        recommendations.append("ðŸŽ¯ **Prediction Diversity**: All predictions are the same type. Try uploading different types of images.")
    
    if conf_std < 0.1:
        recommendations.append("ï¿½ **Confidence Variation**: All predictions have similar confidence. Try images of varying quality/clarity.")
    
    if not recommendations:
        recommendations.append("âœ… **Analysis Complete**: Current data shows good fairness metrics")
    
    for rec in recommendations:
        st.write(rec)

# Page Router
if page == "ðŸ©º Main Diagnosis":
    main_diagnosis_page()
elif page == "ðŸ” LIME Explanations":
    lime_explanations_page()
elif page == "ðŸ§  Advanced Explainability":
    advanced_explainability_page()
elif page == "ðŸ”— Blockchain Data":
    blockchain_data_page()
elif page == "âš–ï¸ Advanced Bias Detection":
    advanced_bias_detection_page()

# Sidebar
st.sidebar.title("ðŸ“Š Prediction Statistics")

stats = blockchain.get_prediction_stats()
if stats:
    benign_count = len([s for s in stats if s['prediction'] == 'benign'])
    malignant_count = len([s for s in stats if s['prediction'] == 'malignant'])
    if benign_count > 0:
        st.sidebar.write(f"ðŸŸ¢ Benign: {benign_count}")
    if malignant_count > 0:
        st.sidebar.write(f"ðŸ”´ Malignant: {malignant_count}")
else:
    st.sidebar.info("No data yet")
